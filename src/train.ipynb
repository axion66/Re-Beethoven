{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.preprocess import *\n",
    "#tensor,sr = load_mp3_files(\"../dataset\")\n",
    "tensor = load_audio(\"../dataset/no8/0/audio0.mp3\",config={\n",
    "    \"sr\":8000,\n",
    "    \"hop_length\":512,\n",
    "    \"cut_first\":8000\n",
    "})\n",
    "#for i in tensor:\n",
    "    #print(f\"tensor{i}.shape: {i.shape}\")\n",
    "print(tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.tools.audios import RevSTFT\n",
    "from layers.diffusion.karras import KarrasNoiseAdder\n",
    "# Instantiate the noise adder\n",
    "print(tensor.std())\n",
    "tensor = tensor * 0.5 / tensor.std()\n",
    "print(tensor.std())\n",
    "noise_adder = KarrasNoiseAdder(sigma_data=tensor.std(dim=-1))\n",
    "\n",
    "f = RevSTFT(config={\n",
    "    \"n_fft\":1024,\n",
    "    \"win_len\":1024,\n",
    "    \"hop_length\":512\n",
    "})\n",
    "a,b = f.transform(tensor)\n",
    "x = torch.cat((a.transpose(-1,-2),b.transpose(-1,-2)),dim=-1)\n",
    "print(x.shape)\n",
    "# Assuming you have an input tensor `x` and a list of sigmas\n",
    "num_steps = 30  # Example number of steps\n",
    "#sigmas = KarrasSchedule(sigma_data=tensor.std(dim=-1)).forward(num_steps,device=torch.device(\"cpu\"))  # Create a noise schedule\n",
    "\n",
    "steps = torch.arange(num_steps, dtype=torch.float32) \n",
    "schuduled_sigmas = (\n",
    "    30 ** 0.33333333333\n",
    "    + (steps / (num_steps - 1)) * (1e-6 ** 0.333333333 - 30 ** 0.33333333)\n",
    ") ** 3\n",
    "# sigmas maximum -> minimum, as sampling method goes backward(T to 0)\n",
    "# Although original paper suggested maximum=80, We should go with maximum=0.8~3, as that's expected noise range used in training step is around there. (Also to reduce cost)\n",
    "sigmas = torch.cat((schuduled_sigmas,schuduled_sigmas.new_zeros([1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(sigmas.shape)\n",
    "x_noisy = noise_adder(x, sigmas)  # x is your original input tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noised_returns = noise_adder.noised_x\n",
    "print(noised_returns[0].shape)\n",
    "stft_to_audio = [f.inverse(aa[:,:,:513],aa[:,:,513:]) for aa in noised_returns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stft_to_audio[0].shape)\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "def save_audio(tensor, filename, sample_rate=8000):\n",
    "    # Ensure the tensor is of the right shape\n",
    "    tensor = tensor * 0.0563 / 0.5\n",
    "    if tensor.dim() == 1:\n",
    "        tensor = tensor.unsqueeze(0)  # Add a channel dimension\n",
    "\n",
    "    # Save the audio file\n",
    "    torchaudio.save(filename, tensor, sample_rate)\n",
    "\n",
    "import os\n",
    "os.makedirs(\"audio_samples\",exist_ok=True)\n",
    "for i,audio in enumerate(stft_to_audio):\n",
    "\n",
    "    save_audio(audio, f\"audio_samples/output_audio{i}.wav\", sample_rate=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor.std(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make them into batched x,y. \n",
    "#tensor_stack = torch.cat(tensor,dim=-1)\n",
    "tensor_stack = tensor\n",
    "\n",
    "print(f\"tensor_stack.shape: {tensor_stack.shape}\")\n",
    "\n",
    "ck_len = 512*20 # for first 32 seconds, we predict the next 32 seconds (sampling rate = 8000)\n",
    "\n",
    "chunks = create_overlapping_chunks_tensor(tensor_stack,chunk_len=ck_len)\n",
    "print(chunks.shape) #torch.Size([706, 96000])\n",
    "x= chunks\n",
    "print(f\"x: {x.shape}\")\n",
    "\n",
    "\n",
    "indices = torch.randperm(x.size(0))\n",
    "\n",
    "shuffled_x = x[indices]\n",
    "\n",
    "dSet = {\n",
    "    'x': shuffled_x[:3350,:],\n",
    "    'x_test': shuffled_x[3450:,:],\n",
    "}\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "trainDataset,testDataset = TensorDataset(dSet['x']),TensorDataset(dSet['x_test'])\n",
    "dLoader,dLoader_test = DataLoader(trainDataset,batch_size=32,shuffle=True),DataLoader(testDataset,batch_size=32,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from layers.core import net\n",
    "LR=1e-4\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = net(sequence_length=512*20,num_blocks=6,activation='silu').to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR,)\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "wandb.init(project=\"audio-gen\", config={\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": len(dLoader),  # assuming dLoader gives one batch per step\n",
    "    \"learning_rate\": LR,\n",
    "    \"device\": \"cuda RTX 3080 Ti\",\n",
    "    \"ck_len\": ck_len,\n",
    "    \"num_blocks\": 6,\n",
    "    },\n",
    "    name=f\"run_{current_time}\"\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i,(inputs, labels) in enumerate(dLoader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        if torch.any(torch.isnan(inputs)) or torch.any(torch.isnan(labels)):\n",
    "            print(\"Input or labels contain NaN values.\")\n",
    "            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        wandb.log({'single_loss': loss.item()})\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dLoader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    wandb.log({\"epoch_train_loss\": epoch_loss})\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'models/modelDict_epoch_{epoch+1}.pth')\n",
    "        torch.save(model, f'models/model_epoch_{epoch+1}.pth')\n",
    "        model.eval() \n",
    "        running_loss_test = 0.0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in dLoader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and torch.any(torch.isnan(param.grad)):\n",
    "                        print(f\"Gradient for {name} contains NaN values.\")\n",
    "                        \n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss_test += loss.item()\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Average validation loss for the epoch\n",
    "        epoch_test_loss = running_loss_test / len(dLoader_test)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        wandb.log({\"eval_loss\":epoch_test_loss})    \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_test_loss:.4f}')\n",
    "    \n",
    "# At the end, you can plot the losses if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "from layers.core import net\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = torch.load(\"model_epoch_45.pth\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Assuming dLoader_test is your test dataloader and each input/label has shape (1, L)\n",
    "output_folder = \"eval_audio\"  # Folder to store audio files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch_idx, (inputs, labels) in enumerate(dLoader_test):\n",
    "        inputs = inputs.to(device)  # Shape (1, L)\n",
    "        labels = labels.to(device)  # Shape (1, L)\n",
    "\n",
    "        outputs = model(inputs)  # Shape (1, L)\n",
    "        concatenated_audio = torch.cat((inputs, outputs,labels), dim=1)  # Shape (1, 3*L)\n",
    "\n",
    "        concatenated_audio = concatenated_audio.cpu().detach()\n",
    "        file_name = f\"{output_folder}/audio_batch_{batch_idx}.wav\"\n",
    "        torchaudio.save(file_name, concatenated_audio, sample_rate=SR)\n",
    "\n",
    "        print(f\"Saved concatenated audio for batch {batch_idx} to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from encoder.utils import convert_audio\n",
    "import torchaudio\n",
    "import torch\n",
    "from decoder.pretrained import WavTokenizer\n",
    "\n",
    "device=torch.device('cpu')\n",
    "\n",
    "config_path = \"./configs/xxx.yaml\"\n",
    "model_path = \"./xxx.ckpt\"\n",
    "\n",
    "wavtokenizer = WavTokenizer.from_pretrained0802(config_path, model_path)\n",
    "wavtokenizer = wavtokenizer.to(device)\n",
    "\n",
    "wav, sr = torchaudio.load(audio_path)\n",
    "wav = convert_audio(wav, sr, 24000, 1) \n",
    "bandwidth_id = torch.tensor([0])\n",
    "wav=wav.to(device)\n",
    "_,discrete_code= wavtokenizer.encode_infer(wav, bandwidth_id=bandwidth_id)\n",
    "print(discrete_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 95267657\n",
      "Total number of parameters: 239092376\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def print_model_size(model):\n",
    "    \"\"\"\n",
    "    Print the total number of parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model whose size you want to print.\n",
    "    \"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "from layers.core2 import UNetWithMHA\n",
    "from layers.core import net\n",
    "# Create an instance of the model\n",
    "\n",
    "model = UNetWithMHA(config={\n",
    "    'seq_len': 380000,\n",
    "    'n_fft':2048,\n",
    "    'win_len':2048,\n",
    "    'hop_length':20,\n",
    "})\n",
    "\n",
    "model2 = net(config={\n",
    "    'seq_len': 380000,\n",
    "    'n_fft':2048,\n",
    "    'win_len':2048,\n",
    "    'hop_length':20,\n",
    "    'num_blocks':6\n",
    "})\n",
    "\n",
    "# Call the function to print model parameters\n",
    "print_model_size(model)\n",
    "print_model_size(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3740)\n",
      "tensor(-0.5399)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "# instantiate the positional embedding in your transformer and pass to all your attention layers\n",
    "\n",
    "rotary_emb = RotaryEmbedding(dim = 32)\n",
    "\n",
    "# mock queries and keys - dimensions should end with (seq_len, feature dimension), and any number of preceding dimensions (batch, heads, etc)\n",
    "\n",
    "q = torch.randn(1, 8, 1024, 64) # queries - (batch, heads, seq len, dimension of head)\n",
    "k = torch.randn(1, 8, 1024, 64) # keys\n",
    "\n",
    "# apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n",
    "print(q[0,1,1,1])\n",
    "qq = rotary_emb.rotate_queries_or_keys(q)\n",
    "kk = rotary_emb.rotate_queries_or_keys(k)\n",
    "print(qq[0,1,1,1])\n",
    "# then do your attention with your queries (q) and keys (k) as usual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (qq-q)\n",
    "ff = (kk - k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial q sample value: 2.1315560340881348\n",
      "Initial q range: (-4.752716064453125, 4.391259670257568)\n",
      "Rotated q sample value: 0.9357888698577881\n",
      "Rotated q range: (-4.752716064453125, 4.988086700439453)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "# Instantiate the positional embedding in your transformer\n",
    "rotary_emb = RotaryEmbedding(dim=16)\n",
    "\n",
    "# Mock queries and keys\n",
    "q = torch.randn(1, 8, 1024, 64)  # queries\n",
    "k = torch.randn(1, 8, 1024, 64)  # keys\n",
    "\n",
    "# Print initial sample value and range\n",
    "print(\"Initial q sample value:\", q[0, 1, 1, 1].item())\n",
    "print(\"Initial q range:\", (q.min().item(), q.max().item()))\n",
    "\n",
    "# Apply rotations\n",
    "qq = rotary_emb.rotate_queries_or_keys(q)\n",
    "kk = rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "# Print post-rotation sample value and range\n",
    "print(\"Rotated q sample value:\", qq[0, 1, 1, 1].item())\n",
    "print(\"Rotated q range:\", (qq.min().item(), qq.max().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
