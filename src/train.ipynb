{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import *\n",
    "tensor,sr = load_mp3_files(\"../dataset\")\n",
    "\n",
    "for i in tensor:\n",
    "    print(f\"tensor{i}.shape: {i.shape}\")\n",
    "print(f\"Sampling rate: {sr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make them into batched x,y. \n",
    "tensor_stack = torch.cat(tensor,dim=-1)\n",
    "print(f\"tensor_stack.shape: {tensor_stack.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_len = 8000*10 # for first 10 seconds, we predict the next 10 seconds (sampling rate = 8000)\n",
    "\n",
    "chunks = create_overlapping_chunks_tensor(tensor_stack,chunk_len=ck_len)\n",
    "print(chunks.shape) #torch.Size([706, 96000])\n",
    "x,y = chunks[:,ck_len//2:], chunks[:,ck_len//2:]\n",
    "print(f\"x: {x.shape}\")\n",
    "print(f\"y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = torch.randperm(x.size(0))\n",
    "\n",
    "shuffled_x,shuffled_y = x[indices],y[indices]\n",
    "\n",
    "dSet = {\n",
    "    'x': shuffled_x[:700,:],\n",
    "    'y': shuffled_y[:700,:],\n",
    "    'x_test': shuffled_x[700:,:],\n",
    "    'y_test': shuffled_y[700:,:],\n",
    "}\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "trainDataset,testDataset = TensorDataset(dSet['x'],dSet['y']),TensorDataset(dSet['x_test'],dSet['y_test'])\n",
    "dLoader,dLoader_test = DataLoader(trainDataset,batch_size=1,shuffle=True),DataLoader(testDataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from layers.vaeNet import net\n",
    "device = torch.device('cuda:0')\n",
    "model = net(sequence_length=8000*5,num_blocks=4,activation='swish').to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,)\n",
    "# Training settings\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i,(inputs, labels) in enumerate(dLoader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        if torch.any(torch.isnan(inputs)) or torch.any(torch.isnan(labels)):\n",
    "            print(\"Input or labels contain NaN values.\")\n",
    "            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=300.0)\n",
    "        optimizer.step()\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.any(torch.isnan(param.grad)):\n",
    "                print(f\"Gradient for {name} contains NaN values.\")\n",
    "                \n",
    "        if (i%10 == 0):\n",
    "            print(f\"loss: {loss.item()}\")\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dLoader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "    torch.save(model.state_dict(), f'modelDict_epoch_{epoch+1}.pth')\n",
    "    torch.save(model, f'model_epoch_{epoch+1}.pth')\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval() \n",
    "        running_loss_test = 0.0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in dLoader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and torch.any(torch.isnan(param.grad)):\n",
    "                        print(f\"Gradient for {name} contains NaN values.\")\n",
    "                        \n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss_test += loss.item()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Average validation loss for the epoch\n",
    "        epoch_test_loss = running_loss_test / len(dLoader_test)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "\n",
    "        # Print validation loss for this epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_test_loss:.4f}')\n",
    "    \n",
    "# At the end, you can plot the losses if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   \n",
    "    demo for ensuring architecture.\n",
    "    from layers.vaeNet import net\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    device = torch.device(\"mps\")\n",
    "    model = net(sequence_length=8000*10,num_blocks=4,activation='swish').to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=3e-4,)\n",
    "\n",
    "    example_tensor = torch.ones((1,1,8000*10)).to(device)\n",
    "    out = model(example_tensor)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from layers.vaeNet import net\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = net(sequence_length=8000*5,num_blocks=4,activation='swish').to(device)\n",
    "\n",
    "def get_model_size(model):\n",
    "    # Get total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Assuming parameters are float32 (4 bytes)\n",
    "    size_in_bytes = total_params * 4\n",
    "    \n",
    "    # Convert to GB\n",
    "    size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "    \n",
    "    return size_in_gb\n",
    "\n",
    "# Example usage with your model\n",
    "model_size_gb = get_model_size(model)\n",
    "print(f\"Model size: {model_size_gb:.4f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.load(\"../models/model_epoch_2.pth\",map_location=torch.device('cpu')).to(torch.device(\"cpu\"))\n",
    "f.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = torch.ones((1,1,40000))\n",
    "out = f(ff)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_params_by_layer(model):\n",
    "    total_params = 0\n",
    "    \n",
    "    print(f\"{'Layer Name':<30} {'Parameters':<20}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        num_params = param.numel()  # Total number of elements in the parameter tensor\n",
    "        total_params += num_params\n",
    "        print(f\"{name:<30} {num_params:<20}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "\n",
    "# Example usage with your model\n",
    "print_model_params_by_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1025, 40])\n",
      "torch.Size([1, 1025, 40])\n",
      "torch.Size([1, 1, 39936])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/nnAudio/Spectrogram.py:4: Warning: importing Spectrogram subpackage will be deprecated soon. You should import the feature extractor from the feature subpackage. See actual documentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from layers.vaeNet import net,TorchSTFT\n",
    "import torch\n",
    "f =TorchSTFT()\n",
    "\n",
    "\n",
    "tensor = torch.ones((1,40000))\n",
    "\n",
    "mag,phase = f.transform(tensor)\n",
    "\n",
    "print(mag.shape)\n",
    "print(phase.shape)\n",
    "\n",
    "\n",
    "ff = f.inverse(mag,phase)\n",
    "\n",
    "print(ff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = f(tensor)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Original signal\n",
    "x = torch.randn(40000)  # 1D tensor (e.g., 1 second of audio at 16 kHz)\n",
    "\n",
    "# Parameters for STFT\n",
    "n_fft = 1024  # Size of FFT\n",
    "hop_length = 512  # Number of samples to move between successive frames\n",
    "\n",
    "# Compute STFT\n",
    "stft_result = torch.stft(x, n_fft=n_fft, hop_length=hop_length,return_complex=True)\n",
    "\n",
    "# Inverse STFT\n",
    "reconstructed_x = torch.istft(stft_result, n_fft=n_fft, hop_length=hop_length, length=x.size(0))\n",
    "\n",
    "# Check shapes\n",
    "print(f'Original shape: {x.shape}, Reconstructed shape: {reconstructed_x.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
