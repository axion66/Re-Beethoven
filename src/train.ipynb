{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensortensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.0139e-05, 3.4890e-05,\n",
      "         3.2205e-05]]).shape: torch.Size([1, 4333568])\n",
      "tensortensor([[ 1.1999e-07, -1.1272e-08, -5.3291e-07,  ...,  2.7773e-11,\n",
      "         -1.3056e-11,  2.6297e-12]]).shape: torch.Size([1, 2189056])\n",
      "tensortensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -4.3125e-05,\n",
      "         -4.0388e-05, -3.3079e-05]]).shape: torch.Size([1, 2569472])\n",
      "tensortensor([[6.1613e-03, 1.7957e-02, 2.6801e-02,  ..., 2.1595e-05, 2.2561e-05,\n",
      "         1.6976e-05]]).shape: torch.Size([1, 1050880])\n",
      "tensortensor([[0.0000, 0.0000, 0.0000,  ..., 0.0005, 0.0005, 0.0005]]).shape: torch.Size([1, 2685440])\n",
      "tensortensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7031e-05, 1.4117e-05,\n",
      "         7.4276e-06]]).shape: torch.Size([1, 3948032])\n",
      "tensortensor([[ 5.7710e-12,  9.9875e-12, -5.6306e-12,  ...,  7.1811e-12,\n",
      "         -3.2290e-12, -1.1336e-11]]).shape: torch.Size([1, 2854912])\n",
      "tensortensor([[ 5.7710e-12,  9.9875e-12, -5.6306e-12,  ...,  3.8519e-08,\n",
      "          6.0563e-08,  8.7639e-08]]).shape: torch.Size([1, 1145088])\n",
      "tensortensor([[ 5.7710e-12,  9.9875e-12, -5.6306e-12,  ..., -6.6278e-12,\n",
      "         -3.3251e-12, -1.1534e-11]]).shape: torch.Size([1, 3811840])\n",
      "tensortensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.7905e-05,\n",
      "         -1.4789e-05, -1.0706e-05]]).shape: torch.Size([1, 4446208])\n",
      "tensortensor([[0., 0., 0.,  ..., 0., 0., 0.]]).shape: torch.Size([1, 2140160])\n",
      "tensortensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -7.7013e-06,\n",
      "         -7.7109e-06, -1.4671e-05]]).shape: torch.Size([1, 2803712])\n",
      "Sampling rate: 8000\n"
     ]
    }
   ],
   "source": [
    "from preprocess import *\n",
    "tensor,sr = load_mp3_files(\"../dataset\")\n",
    "\n",
    "for i in tensor:\n",
    "    print(f\"tensor{i}.shape: {i.shape}\")\n",
    "print(f\"Sampling rate: {sr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_stack.shape: torch.Size([1, 33978368])\n"
     ]
    }
   ],
   "source": [
    "# make them into batched x,y. \n",
    "tensor_stack = torch.cat(tensor,dim=-1)\n",
    "print(f\"tensor_stack.shape: {tensor_stack.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6629, 40960])\n",
      "x: torch.Size([6629, 20480])\n",
      "y: torch.Size([6629, 20480])\n"
     ]
    }
   ],
   "source": [
    "ck_len = 512*600 # for first 32 seconds, we predict the next 32 seconds (sampling rate = 8000)\n",
    "\n",
    "chunks = create_overlapping_chunks_tensor(tensor_stack,chunk_len=ck_len)\n",
    "print(chunks.shape) #torch.Size([706, 96000])\n",
    "x,y = chunks[:,:ck_len//2], chunks[:,ck_len//2:]\n",
    "print(f\"x: {x.shape}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices = torch.randperm(x.size(0))\n",
    "\n",
    "shuffled_x,shuffled_y = x[indices],y[indices]\n",
    "\n",
    "dSet = {\n",
    "    'x': shuffled_x[:3450,:],\n",
    "    'y': shuffled_y[:3450,:],\n",
    "    'x_test': shuffled_x[3450:,:],\n",
    "    'y_test': shuffled_y[3450:,:],\n",
    "}\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "trainDataset,testDataset = TensorDataset(dSet['x'],dSet['y']),TensorDataset(dSet['x_test'],dSet['y_test'])\n",
    "dLoader,dLoader_test = DataLoader(trainDataset,batch_size=1,shuffle=True),DataLoader(testDataset,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from layers.main_model import net\n",
    "LR=1e-4\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = net(sequence_length=512*300,num_blocks=6,activation='silu').to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR,)\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/ssongjinseobgmail.com/coding/Re-Beethoven/src/wandb/run-20241016_054054-5m5c0kyy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hellomynameisshady/audio-gen/runs/5m5c0kyy' target=\"_blank\">run_2024-10-16_05-40-54</a></strong> to <a href='https://wandb.ai/hellomynameisshady/audio-gen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hellomynameisshady/audio-gen' target=\"_blank\">https://wandb.ai/hellomynameisshady/audio-gen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hellomynameisshady/audio-gen/runs/5m5c0kyy' target=\"_blank\">https://wandb.ai/hellomynameisshady/audio-gen/runs/5m5c0kyy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.490424806950614e-05\n",
      "loss: 0.002055252203717828\n",
      "loss: 0.003938093315809965\n",
      "loss: 0.006290995515882969\n",
      "loss: 0.001245544059202075\n",
      "loss: 0.0005569446948356926\n",
      "loss: 0.00013573670003097504\n",
      "loss: 0.0008337479084730148\n",
      "loss: 0.0019607783760875463\n",
      "loss: 0.006679121404886246\n",
      "loss: 0.0008255488937720656\n",
      "loss: 0.0002555732789915055\n",
      "loss: 0.00013253148063085973\n",
      "loss: 0.004887404385954142\n",
      "loss: 0.003138697938993573\n",
      "loss: 0.0011610668152570724\n",
      "loss: 0.004229875281453133\n",
      "loss: 2.242262962681707e-05\n",
      "loss: 0.008252294734120369\n",
      "loss: 0.001216390635818243\n",
      "loss: 0.002600605133920908\n",
      "loss: 0.0013715634122490883\n",
      "loss: 0.00019980059005320072\n",
      "loss: 8.914861973607913e-05\n",
      "loss: 0.005425921641290188\n",
      "loss: 0.00017995423695538193\n",
      "loss: 0.005412047263234854\n",
      "loss: 0.005595244932919741\n",
      "loss: 5.37126325070858e-05\n",
      "loss: 0.0005044179852120578\n",
      "loss: 0.00019768928177654743\n",
      "loss: 0.00032757720327936113\n",
      "loss: 0.00029478437500074506\n",
      "loss: 0.0017181731527671218\n",
      "loss: 0.000912757299374789\n",
      "loss: 9.438435517949983e-05\n",
      "loss: 0.0017505644354969263\n",
      "loss: 0.005326526239514351\n",
      "loss: 0.0021752058528363705\n",
      "loss: 0.0006556467269547284\n",
      "loss: 0.0036039804108440876\n",
      "loss: 0.0058807795867323875\n",
      "loss: 0.0025470787659287453\n",
      "loss: 0.00022546474065165967\n",
      "loss: 0.0029789917171001434\n",
      "loss: 0.001386835821904242\n",
      "loss: 0.0008149450877681375\n",
      "loss: 0.00224048038944602\n",
      "loss: 0.001940742484293878\n",
      "loss: 0.007066869642585516\n",
      "loss: 0.0015623464714735746\n",
      "loss: 0.0013791646342724562\n",
      "loss: 0.0009554381249472499\n",
      "loss: 0.00024790721363388\n",
      "loss: 0.006743418984115124\n",
      "loss: 0.002229119883850217\n",
      "loss: 0.00014219782315194607\n",
      "loss: 0.004465664271265268\n",
      "loss: 0.0019437711453065276\n",
      "loss: 0.00016704335575923324\n",
      "loss: 0.001084513496607542\n",
      "loss: 0.0002051129995379597\n",
      "loss: 0.00016161330859176815\n",
      "loss: 0.007807242684066296\n",
      "loss: 0.00664726784452796\n",
      "loss: 0.0032990104518830776\n",
      "loss: 0.0014226827770471573\n",
      "loss: 0.002629241207614541\n",
      "loss: 0.0007250153576023877\n",
      "loss: 0.001175487064756453\n",
      "loss: 0.0002666764776222408\n",
      "loss: 0.003947914112359285\n",
      "loss: 0.0057646059431135654\n",
      "loss: 0.0005779903149232268\n",
      "loss: 0.0012974936980754137\n",
      "loss: 0.0002928761241491884\n",
      "loss: 0.0031794074457138777\n",
      "loss: 0.0029319575987756252\n",
      "loss: 0.001369537552818656\n",
      "loss: 0.0022607925347983837\n",
      "loss: 0.0016664199065417051\n",
      "loss: 0.003224364249035716\n",
      "loss: 0.0014531553024426103\n",
      "loss: 0.00012188350956421345\n",
      "loss: 0.003671633778139949\n",
      "loss: 0.002235685708001256\n",
      "loss: 0.009746464900672436\n",
      "loss: 0.0024878159165382385\n",
      "loss: 0.003769360017031431\n",
      "loss: 0.003804213600233197\n",
      "loss: 0.0004087663837708533\n",
      "loss: 0.00048094746307469904\n",
      "loss: 0.0003480812592897564\n",
      "loss: 0.0011376275215297937\n",
      "loss: 0.0035775683354586363\n",
      "loss: 0.0001964156690519303\n",
      "loss: 0.00017453536565881222\n",
      "loss: 0.0002423248515697196\n",
      "loss: 8.857983630150557e-06\n",
      "loss: 0.003035252448171377\n",
      "loss: 0.001778086880221963\n",
      "loss: 0.002850781660526991\n",
      "loss: 0.0038196153473109007\n",
      "loss: 0.0019287128234282136\n",
      "loss: 0.00173426722176373\n",
      "loss: 0.002830040641129017\n",
      "loss: 0.0011011675233021379\n",
      "loss: 7.364792509179097e-06\n",
      "loss: 0.0037337143439799547\n",
      "loss: 0.0012204756494611502\n",
      "loss: 0.00012355025683064014\n",
      "loss: 0.0004367535875644535\n",
      "loss: 0.0021286664996296167\n",
      "loss: 0.00014186717453412712\n",
      "loss: 0.0006199906347319484\n",
      "loss: 0.0007163137197494507\n",
      "loss: 0.002239201683551073\n",
      "loss: 0.00015970945241861045\n",
      "loss: 0.0034755084197968245\n",
      "loss: 0.0031542356591671705\n",
      "loss: 0.00018864322919398546\n",
      "loss: 0.019878443330526352\n",
      "loss: 9.991205297410488e-05\n",
      "loss: 0.0003433672827668488\n",
      "loss: 0.00569786736741662\n",
      "loss: 0.00018207992252428085\n",
      "loss: 0.0024841343984007835\n",
      "loss: 0.002361899707466364\n",
      "loss: 0.0004710980283562094\n",
      "loss: 0.0028737427201122046\n",
      "loss: 2.057043531067393e-07\n",
      "loss: 0.0009606989333406091\n",
      "loss: 0.0004899749765172601\n",
      "loss: 0.0023656191769987345\n",
      "loss: 0.00015470923972316086\n",
      "loss: 0.0016945572569966316\n",
      "loss: 0.004866794683039188\n",
      "loss: 0.0009394263615831733\n",
      "loss: 0.007656038738787174\n",
      "loss: 0.005177012644708157\n",
      "loss: 0.006943541578948498\n",
      "loss: 4.9059326556744054e-05\n",
      "loss: 0.00012563102063722908\n",
      "loss: 0.0020352902356535196\n",
      "loss: 0.0067073991522192955\n",
      "loss: 0.0011146385222673416\n",
      "loss: 0.000998280942440033\n",
      "loss: 0.00017665117047727108\n",
      "loss: 0.00034498778404667974\n",
      "loss: 0.007559735327959061\n",
      "loss: 0.0021285689435899258\n",
      "loss: 0.00012085136404493824\n",
      "loss: 0.00986754521727562\n",
      "loss: 0.00023743156634736806\n",
      "loss: 0.00018298975192010403\n",
      "loss: 0.0007483665831387043\n",
      "loss: 0.0014797437470406294\n",
      "loss: 0.0003416709369048476\n",
      "loss: 0.0015979139134287834\n",
      "loss: 5.7010289310710505e-05\n",
      "loss: 8.334672747878358e-05\n",
      "loss: 0.005452178884297609\n",
      "loss: 0.0002487925230525434\n",
      "loss: 0.00017195990949403495\n",
      "loss: 0.00473372545093298\n",
      "loss: 0.005044466350227594\n",
      "loss: 0.0006100136088207364\n",
      "loss: 6.642825610470027e-05\n",
      "loss: 0.0031284044962376356\n",
      "loss: 0.00014356733299791813\n",
      "loss: 9.660523210186511e-05\n",
      "loss: 0.0005588122876361012\n",
      "loss: 0.0021865416783839464\n",
      "loss: 0.003916486166417599\n",
      "loss: 2.3687853456522134e-07\n",
      "loss: 0.00034963933285325766\n",
      "loss: 0.00017615820979699492\n",
      "loss: 0.00025273513165302575\n",
      "loss: 0.0001872653083410114\n",
      "loss: 0.007779707200825214\n",
      "loss: 0.00016878331371117383\n",
      "loss: 0.0028029335662722588\n",
      "loss: 0.0007138564833439887\n",
      "loss: 0.00013517036859411746\n",
      "loss: 6.657704943791032e-05\n",
      "loss: 0.00012791762128472328\n",
      "loss: 0.005456564016640186\n",
      "loss: 0.0006760678952559829\n",
      "loss: 0.00015487939526792616\n",
      "loss: 9.359065006719902e-05\n",
      "loss: 0.0004849615506827831\n",
      "loss: 0.0009267540881410241\n",
      "loss: 0.0011379113420844078\n",
      "loss: 0.0004683741426561028\n",
      "loss: 0.008618494495749474\n",
      "loss: 0.0014528547180816531\n",
      "loss: 0.0023746087681502104\n",
      "loss: 0.004894420504570007\n",
      "loss: 0.001520730322226882\n",
      "loss: 5.7866156566888094e-05\n",
      "loss: 0.004934812895953655\n",
      "loss: 0.0019625171553343534\n",
      "loss: 0.0002828061697073281\n",
      "loss: 0.00108353013638407\n",
      "loss: 0.004886091686785221\n",
      "loss: 0.002946352120488882\n",
      "loss: 0.007145444396883249\n",
      "loss: 0.00011212801473448053\n",
      "loss: 0.00029690490919165313\n",
      "loss: 0.00025190640008077025\n",
      "Epoch [1/500], Training Loss: 0.0021\n",
      "Epoch [1/500], Validation Loss: 0.0020\n",
      "loss: 0.003025173209607601\n",
      "loss: 0.0005378772038966417\n",
      "loss: 0.00012314556806813926\n",
      "loss: 0.00174519547726959\n",
      "loss: 0.009309539571404457\n",
      "loss: 0.004456575959920883\n",
      "loss: 0.0018221454229205847\n",
      "loss: 0.0005865569110028446\n",
      "loss: 0.0003393411752767861\n",
      "loss: 0.00409745192155242\n",
      "loss: 0.0002470545587129891\n",
      "loss: 0.004174609202891588\n",
      "loss: 0.0032589812763035297\n",
      "loss: 0.0024076025001704693\n",
      "loss: 0.0006334330537356436\n",
      "loss: 0.0003650925646070391\n",
      "loss: 0.0001503102103015408\n",
      "loss: 0.00075852673035115\n",
      "loss: 6.637909973505884e-05\n",
      "loss: 0.0027137896977365017\n",
      "loss: 0.0035899982322007418\n",
      "loss: 0.0003734016208909452\n",
      "loss: 0.0012440000427886844\n",
      "loss: 0.001532010268419981\n",
      "loss: 0.0035727727226912975\n",
      "loss: 0.0007015839219093323\n",
      "loss: 0.0002162909077014774\n",
      "loss: 0.0007502640364691615\n",
      "loss: 0.0033196615986526012\n",
      "loss: 8.086020670816652e-07\n",
      "loss: 0.0013420257018879056\n",
      "loss: 0.0050420621410012245\n",
      "loss: 0.002944227773696184\n",
      "loss: 0.007112398743629456\n",
      "loss: 0.00018255403847433627\n",
      "loss: 0.0022250735200941563\n",
      "loss: 0.0017456726636737585\n",
      "loss: 0.00023187845363281667\n",
      "loss: 0.005845373962074518\n",
      "loss: 0.00012649668497033417\n",
      "loss: 0.007805631496012211\n",
      "loss: 0.0022407914511859417\n",
      "loss: 0.0010131103917956352\n",
      "loss: 8.270746434391185e-07\n",
      "loss: 0.0004373195697553456\n",
      "loss: 0.0011409848229959607\n",
      "loss: 0.0005775062018074095\n",
      "loss: 0.0005506256711669266\n",
      "loss: 0.002696230076253414\n",
      "loss: 8.308074757223949e-05\n",
      "loss: 0.0002962023136205971\n",
      "loss: 0.004408224485814571\n",
      "loss: 0.000587374612223357\n",
      "loss: 0.002865239977836609\n",
      "loss: 0.005787841975688934\n",
      "loss: 0.00012602124479599297\n",
      "loss: 0.0006057329010218382\n",
      "loss: 0.00017149379709735513\n",
      "loss: 0.0008515174267813563\n",
      "loss: 0.00012008979683741927\n",
      "loss: 0.0019131986191496253\n",
      "loss: 0.0037724212743341923\n",
      "loss: 0.0001398931781295687\n",
      "loss: 7.02164979884401e-05\n",
      "loss: 0.00455938745290041\n",
      "loss: 0.001945513067767024\n",
      "loss: 0.001822464750148356\n",
      "loss: 0.003963404335081577\n",
      "loss: 0.007167496718466282\n",
      "loss: 4.092098606633954e-05\n",
      "loss: 0.00030776316998526454\n",
      "loss: 0.004211231134831905\n",
      "loss: 0.002300336491316557\n",
      "loss: 0.00473563140258193\n",
      "loss: 3.726905561052263e-05\n",
      "loss: 0.0041919248178601265\n",
      "loss: 0.0007303230231627822\n",
      "loss: 0.003010025480762124\n",
      "loss: 0.0001012940047075972\n",
      "loss: 0.008594976738095284\n",
      "loss: 0.0003657870984170586\n",
      "loss: 0.0009071960230357945\n",
      "loss: 0.0013202218106016517\n",
      "loss: 0.0004385759530123323\n",
      "loss: 6.669694130323478e-07\n",
      "loss: 0.007776586804538965\n",
      "loss: 0.0016662065172567964\n",
      "loss: 0.0002497477107681334\n",
      "loss: 0.0013805131893604994\n",
      "loss: 0.0026080324314534664\n",
      "loss: 0.0001499740465078503\n",
      "loss: 0.000294097961159423\n",
      "loss: 0.004794158507138491\n",
      "loss: 0.0008139811689034104\n",
      "loss: 0.00022261652338784188\n",
      "loss: 0.000385133404051885\n",
      "loss: 0.0021998672746121883\n",
      "loss: 0.0004900848143734038\n",
      "loss: 0.008148161694407463\n",
      "loss: 0.00021567693329416215\n",
      "loss: 0.00014942785492166877\n",
      "loss: 5.941039489698596e-05\n",
      "loss: 0.0012448134366422892\n",
      "loss: 0.0018075171392410994\n",
      "loss: 0.005294010974466801\n",
      "loss: 0.00019840158347506076\n",
      "loss: 0.0006010173819959164\n",
      "loss: 0.00015410655760206282\n",
      "loss: 0.00020177484839223325\n",
      "loss: 7.80072805355303e-05\n",
      "loss: 0.0008400783990509808\n",
      "loss: 0.00011084794823545963\n",
      "loss: 0.00048542473814450204\n",
      "loss: 0.0027386569418013096\n",
      "loss: 0.002748040948063135\n",
      "loss: 0.001429916126653552\n",
      "loss: 0.00013486701936926693\n",
      "loss: 0.0017135757952928543\n",
      "loss: 0.007778583560138941\n",
      "loss: 0.009328615851700306\n",
      "loss: 0.003936484456062317\n",
      "loss: 0.00501004233956337\n",
      "loss: 0.00010616857616696507\n",
      "loss: 0.0003185949462931603\n",
      "loss: 0.003006553975865245\n",
      "loss: 0.00013530005526263267\n",
      "loss: 0.005604741163551807\n",
      "loss: 0.00119023269508034\n",
      "loss: 0.000150579639011994\n",
      "loss: 0.0011056826915591955\n",
      "loss: 0.0002444290730636567\n",
      "loss: 0.0002713763969950378\n",
      "loss: 0.005579229444265366\n",
      "loss: 0.0007391475373879075\n",
      "loss: 0.00029633677331730723\n",
      "loss: 0.009827161207795143\n",
      "loss: 0.002169278683140874\n",
      "loss: 0.0004164209240116179\n",
      "loss: 9.501784370513633e-05\n",
      "loss: 0.0004362096660770476\n",
      "loss: 0.004381027538329363\n",
      "loss: 0.0021438829135149717\n",
      "loss: 0.0024075352121144533\n",
      "loss: 0.0007192789344117045\n",
      "loss: 0.005790029186755419\n",
      "loss: 0.005518639460206032\n",
      "loss: 0.0025323075242340565\n",
      "loss: 0.00021854527585674077\n",
      "loss: 0.0036108456552028656\n",
      "loss: 0.007537239696830511\n",
      "loss: 0.0013618656666949391\n",
      "loss: 0.0030157032888382673\n",
      "loss: 0.0018678943160921335\n",
      "loss: 0.002034433651715517\n",
      "loss: 4.3596428440650925e-05\n",
      "loss: 3.067951911361888e-05\n",
      "loss: 0.0036566685885190964\n",
      "loss: 0.0011544160079210997\n",
      "loss: 0.0010791766690090299\n",
      "loss: 0.0008479662355966866\n",
      "loss: 0.0018559398595243692\n",
      "loss: 0.004760569427162409\n",
      "loss: 0.00021484436001628637\n",
      "loss: 0.003757736412808299\n",
      "loss: 0.004355986602604389\n",
      "loss: 0.00019350959337316453\n",
      "loss: 0.0027046159375458956\n",
      "loss: 0.0024501753505319357\n",
      "loss: 0.0030655425507575274\n",
      "loss: 0.001194151584059\n",
      "loss: 0.0001162074986496009\n",
      "loss: 0.0037615045439451933\n",
      "loss: 0.0020336280576884747\n",
      "loss: 0.00024330495216418058\n",
      "loss: 0.00032662315061315894\n",
      "loss: 0.0004289853386580944\n",
      "loss: 0.0002807601704262197\n",
      "loss: 0.006345959845930338\n",
      "loss: 0.002191559411585331\n",
      "loss: 0.002333698095753789\n",
      "loss: 0.0007442607311531901\n",
      "loss: 0.003588710678741336\n",
      "loss: 0.00015503616305068135\n",
      "loss: 0.005713388789445162\n",
      "loss: 0.0008259541355073452\n",
      "loss: 0.002590044168755412\n",
      "loss: 0.00536032859236002\n",
      "loss: 0.0017160044517368078\n",
      "loss: 0.0005822937237098813\n",
      "loss: 0.0019064873922616243\n",
      "loss: 0.0003402996517252177\n",
      "loss: 0.0002687194210011512\n",
      "loss: 0.002181775402277708\n",
      "loss: 0.0010888910619542003\n",
      "loss: 0.005424897186458111\n",
      "loss: 0.0010003212373703718\n",
      "loss: 0.0047713713720440865\n",
      "loss: 0.00028326481697149575\n",
      "loss: 0.0003006828192155808\n",
      "loss: 0.003942166920751333\n",
      "loss: 6.810491322539747e-05\n",
      "loss: 0.0045854742638766766\n",
      "loss: 0.005331828258931637\n",
      "loss: 0.00023590859200339764\n",
      "loss: 0.00022821416496299207\n",
      "loss: 0.0003736040962394327\n",
      "loss: 0.00018185419321525842\n",
      "loss: 0.00034674903145059943\n",
      "loss: 0.004512484651058912\n",
      "loss: 0.00013265651068650186\n",
      "Epoch [2/500], Training Loss: 0.0021\n",
      "loss: 0.0015347315929830074\n",
      "loss: 0.0026436722837388515\n",
      "loss: 0.0002933998475782573\n",
      "loss: 0.0017513990169391036\n",
      "loss: 0.00012410475756041706\n",
      "loss: 0.0020918597001582384\n",
      "loss: 0.004793777596205473\n",
      "loss: 0.00013554739416576922\n",
      "loss: 0.007135865278542042\n",
      "loss: 0.002282691653817892\n",
      "loss: 0.0016637600492686033\n",
      "loss: 0.003231113776564598\n",
      "loss: 0.00012534628331195563\n",
      "loss: 0.009246988222002983\n",
      "loss: 0.003558275057002902\n",
      "loss: 0.0009752556798048317\n",
      "loss: 7.093559543136507e-05\n",
      "loss: 0.00046096500591374934\n",
      "loss: 0.00022213775082491338\n",
      "loss: 0.004025702364742756\n",
      "loss: 0.0001783789339242503\n",
      "loss: 6.973622657824308e-05\n",
      "loss: 0.0005328023689799011\n",
      "loss: 0.004086917731910944\n",
      "loss: 0.00693342974409461\n",
      "loss: 0.005129245109856129\n",
      "loss: 0.0007608958403579891\n",
      "loss: 0.00024363771080970764\n",
      "loss: 0.0001170369578176178\n",
      "loss: 0.0024553306866437197\n",
      "loss: 0.0007931681466288865\n",
      "loss: 0.001893158070743084\n",
      "loss: 9.122397750616074e-05\n",
      "loss: 0.0024185641668736935\n",
      "loss: 0.0002328005648450926\n",
      "loss: 0.0023558461107313633\n",
      "loss: 0.0004891652497462928\n",
      "loss: 0.00516433734446764\n",
      "loss: 0.004288848023861647\n",
      "loss: 0.006393240299075842\n",
      "loss: 0.0026176911778748035\n",
      "loss: 0.00022415505372919142\n",
      "loss: 0.002748967381194234\n",
      "loss: 0.00047961444943211973\n",
      "loss: 0.0056584617123007774\n",
      "loss: 0.004632541444152594\n",
      "loss: 0.0001664957671891898\n",
      "loss: 0.00023494537163060158\n",
      "loss: 0.00010977844067383558\n",
      "loss: 0.0012091079261153936\n",
      "loss: 0.0012275066692382097\n",
      "loss: 0.0066160657443106174\n",
      "loss: 0.0013102235971018672\n",
      "loss: 0.003818222787231207\n",
      "loss: 6.479783041868359e-05\n",
      "loss: 0.0003378556575626135\n",
      "loss: 0.002566031413152814\n",
      "loss: 0.00042691471753641963\n",
      "loss: 0.005602695979177952\n",
      "loss: 0.000803641858510673\n",
      "loss: 0.0042031132616102695\n",
      "loss: 0.00012879623682238162\n",
      "loss: 0.005432990379631519\n",
      "loss: 9.667113772593439e-05\n",
      "loss: 0.00018639971676748246\n",
      "loss: 0.0034196549095213413\n",
      "loss: 0.0012206649407744408\n",
      "loss: 0.00015137922309804708\n",
      "loss: 0.00014107560855336487\n",
      "loss: 0.0018232067814096808\n",
      "loss: 0.0012036394327878952\n",
      "loss: 0.0012656867038458586\n",
      "loss: 0.0016576626803725958\n",
      "loss: 0.0011274672579020262\n",
      "loss: 0.0031368539202958345\n",
      "loss: 0.00036807096330448985\n",
      "loss: 0.0026241764426231384\n",
      "loss: 0.0002500245755072683\n",
      "loss: 0.0002466707956045866\n",
      "loss: 0.0043098474852740765\n",
      "loss: 0.00022600106603931636\n",
      "loss: 0.005554645322263241\n",
      "loss: 0.0003534143033903092\n",
      "loss: 5.148778291186318e-05\n",
      "loss: 0.0005073930369690061\n",
      "loss: 0.0018211876740679145\n",
      "loss: 0.00010890294652199373\n",
      "loss: 0.0009443212184123695\n",
      "loss: 6.401038262993097e-05\n",
      "loss: 0.002871882636100054\n",
      "loss: 0.0001393236016156152\n",
      "loss: 0.00029581208946183324\n",
      "loss: 0.00012381086708046496\n",
      "loss: 0.00022230895410757512\n",
      "loss: 0.002153501845896244\n",
      "loss: 0.0010530977742746472\n",
      "loss: 0.0003017628623638302\n",
      "loss: 0.0002272211277158931\n",
      "loss: 0.0008605409529991448\n",
      "loss: 0.0019516472239047289\n",
      "loss: 0.000320090854074806\n",
      "loss: 0.005516523960977793\n",
      "loss: 0.00025447033112868667\n",
      "loss: 0.005804912187159061\n",
      "loss: 0.00020911738101858646\n",
      "loss: 6.51136360829696e-05\n",
      "loss: 0.0014737703604623675\n",
      "loss: 0.007423726376146078\n",
      "loss: 0.00013784057227894664\n",
      "loss: 0.0027641095221042633\n",
      "loss: 0.002286630216985941\n",
      "loss: 0.00022300382261164486\n",
      "loss: 0.0002473275817465037\n",
      "loss: 9.251344454241917e-05\n",
      "loss: 0.00019704087753780186\n",
      "loss: 0.005182265769690275\n",
      "loss: 0.00034793352824635804\n",
      "loss: 0.00040776142850518227\n",
      "loss: 0.0007371028768830001\n",
      "loss: 0.006310889031738043\n",
      "loss: 0.0015815470833331347\n",
      "loss: 0.0012712066527456045\n",
      "loss: 0.005538320634514093\n",
      "loss: 0.0008291870472021401\n",
      "loss: 0.007397704757750034\n",
      "loss: 0.00029592731152661145\n",
      "loss: 0.004633116070181131\n",
      "loss: 0.007302244193851948\n",
      "loss: 0.00031026446959003806\n",
      "loss: 0.006117336917668581\n",
      "loss: 0.007794187869876623\n",
      "loss: 1.9466988305794075e-05\n",
      "loss: 0.00022191062453202903\n",
      "loss: 0.002009205985814333\n",
      "loss: 0.0001682199799688533\n",
      "loss: 0.0001504201936768368\n",
      "loss: 0.0038221939466893673\n",
      "loss: 0.00015386019367724657\n",
      "loss: 0.000346147280652076\n",
      "loss: 0.004290870390832424\n",
      "loss: 0.0013365261256694794\n",
      "loss: 0.000337166158715263\n",
      "loss: 0.00038054288597777486\n",
      "loss: 0.0012733389157801867\n",
      "loss: 0.006653233431279659\n",
      "loss: 0.0023394650779664516\n",
      "loss: 0.004088063724339008\n",
      "loss: 0.0020609793718904257\n",
      "loss: 0.0023692003451287746\n",
      "loss: 0.00019136031914968044\n",
      "loss: 0.000583329820074141\n",
      "loss: 0.0036702777724713087\n",
      "loss: 0.00040644005639478564\n",
      "loss: 0.0038926596753299236\n",
      "loss: 0.00017113311332650483\n",
      "loss: 0.0021522068418562412\n",
      "loss: 7.281619218701962e-06\n",
      "loss: 0.000508577621076256\n",
      "loss: 0.0011164713650941849\n",
      "loss: 0.0018171683186665177\n",
      "loss: 0.004908291157335043\n",
      "loss: 0.00015146448276937008\n",
      "loss: 0.0004097340570297092\n",
      "loss: 0.0021396283991634846\n",
      "loss: 0.0006269566947594285\n",
      "loss: 0.00028981248033232987\n",
      "loss: 0.006651235278695822\n",
      "loss: 0.00026793513097800314\n",
      "loss: 0.0018686701077967882\n",
      "loss: 0.0036074756644666195\n",
      "loss: 0.002805293072015047\n",
      "loss: 0.0005431022145785391\n",
      "loss: 0.00013228380703367293\n",
      "loss: 0.0029310695827007294\n",
      "loss: 0.0002107474283548072\n",
      "loss: 0.0008341955835931003\n",
      "loss: 0.0002055223158095032\n",
      "loss: 0.005852035246789455\n",
      "loss: 5.170380973140709e-05\n",
      "loss: 0.0004655167867895216\n",
      "loss: 0.001106659066863358\n",
      "loss: 0.005273062270134687\n",
      "loss: 0.00016096695617306978\n",
      "loss: 0.0027092930395156145\n",
      "loss: 0.0027816747315227985\n",
      "loss: 3.898044269590173e-06\n",
      "loss: 0.0017051935428753495\n",
      "loss: 0.001598113914951682\n",
      "loss: 0.001104624941945076\n",
      "loss: 0.00031381292501464486\n",
      "loss: 0.00022899029136169702\n",
      "loss: 0.0012174021685495973\n",
      "loss: 0.000799138389993459\n",
      "loss: 0.00011704370990628377\n",
      "loss: 0.009216423146426678\n",
      "loss: 0.0001657616812735796\n",
      "loss: 0.00013921817298978567\n",
      "loss: 0.00032991968328133225\n",
      "loss: 0.005319795571267605\n",
      "loss: 0.0034188113640993834\n",
      "loss: 0.004666391760110855\n",
      "loss: 9.165003575617447e-06\n",
      "loss: 0.00012729618174489588\n",
      "loss: 0.00016867804515641183\n",
      "loss: 0.0009011804941110313\n",
      "loss: 0.0010082130320370197\n",
      "loss: 0.005560633726418018\n",
      "loss: 0.002050075214356184\n",
      "loss: 1.4531049146171426e-06\n",
      "loss: 0.0016368230571970344\n",
      "Epoch [3/500], Training Loss: 0.0021\n",
      "loss: 6.51961745461449e-05\n",
      "loss: 0.0005971153150312603\n",
      "loss: 0.001025815145112574\n",
      "loss: 0.0006487244972959161\n",
      "loss: 2.896752266678959e-05\n",
      "loss: 0.004513779655098915\n",
      "loss: 0.001009923405945301\n",
      "loss: 0.0021673892624676228\n",
      "loss: 0.0001102437381632626\n",
      "loss: 0.000315120501909405\n",
      "loss: 0.005739760585129261\n",
      "loss: 0.005365149583667517\n",
      "loss: 0.005275570321828127\n",
      "loss: 0.0011931543704122305\n",
      "loss: 0.004446395672857761\n",
      "loss: 0.0005375385517254472\n",
      "loss: 0.0001681156427366659\n",
      "loss: 0.0004883776418864727\n",
      "loss: 0.0012712093302980065\n",
      "loss: 1.8356735154156922e-06\n",
      "loss: 0.004310918971896172\n",
      "loss: 0.00024289579596370459\n",
      "loss: 0.002899282379075885\n",
      "loss: 1.9609291484812275e-05\n",
      "loss: 0.00044793207780458033\n",
      "loss: 0.0024383445270359516\n",
      "loss: 0.0020126907620579004\n",
      "loss: 2.470006393195945e-06\n",
      "loss: 0.003646585624665022\n",
      "loss: 0.0013312215451151133\n",
      "loss: 0.0004876124730799347\n",
      "loss: 0.00474968459457159\n",
      "loss: 0.0029369902331382036\n",
      "loss: 0.006262138485908508\n",
      "loss: 9.351003973279148e-05\n",
      "loss: 0.0003783219144679606\n",
      "loss: 0.001124026020988822\n",
      "loss: 0.0063463179394602776\n",
      "loss: 0.003888061735779047\n",
      "loss: 0.003478801576420665\n",
      "loss: 0.0012666573747992516\n",
      "loss: 0.003233486320823431\n",
      "loss: 0.007138412445783615\n",
      "loss: 0.00021684747480321676\n",
      "loss: 0.0006345283472910523\n",
      "loss: 0.002886272734031081\n",
      "loss: 0.0015420995187014341\n",
      "loss: 0.0030504504684358835\n",
      "loss: 0.0005554163362830877\n",
      "loss: 6.633038719883189e-05\n",
      "loss: 0.0006931576645001769\n",
      "loss: 0.001100438879802823\n",
      "loss: 0.004832621663808823\n",
      "loss: 6.435949762817472e-05\n",
      "loss: 0.0035609211772680283\n",
      "loss: 0.0002057432138826698\n",
      "loss: 0.0001790039095794782\n",
      "loss: 0.00030463983421213925\n",
      "loss: 0.0005792670417577028\n",
      "loss: 0.0002890294708777219\n",
      "loss: 0.00640955800190568\n",
      "loss: 0.0038022405933588743\n",
      "loss: 0.002734182868152857\n",
      "loss: 0.0006806044839322567\n",
      "loss: 0.0006314808269962668\n",
      "loss: 0.007322415709495544\n",
      "loss: 0.005817005876451731\n",
      "loss: 0.003067251993343234\n",
      "loss: 0.0010585698764771223\n",
      "loss: 0.00046163727529346943\n",
      "loss: 0.000639899808447808\n",
      "loss: 0.0009391886997036636\n",
      "loss: 0.00765759963542223\n",
      "loss: 0.008130762726068497\n",
      "loss: 0.005652481224387884\n",
      "loss: 0.0011367228580638766\n",
      "loss: 0.0007292676018550992\n",
      "loss: 0.0001049960483214818\n",
      "loss: 0.0002502879360690713\n",
      "loss: 0.001935280510224402\n",
      "loss: 8.637038263259456e-05\n",
      "loss: 0.00018324636039324105\n",
      "loss: 0.0060546984896063805\n",
      "loss: 0.004434789065271616\n",
      "loss: 0.0025600700173527002\n",
      "loss: 5.239582242211327e-05\n",
      "loss: 0.0028458223678171635\n",
      "loss: 0.0020547141321003437\n",
      "loss: 0.0001877842441899702\n",
      "loss: 0.0009577737073414028\n",
      "loss: 0.000170165702002123\n",
      "loss: 0.0009912344394251704\n",
      "loss: 0.0021368670277297497\n",
      "loss: 0.000836366496514529\n",
      "loss: 0.006859635002911091\n",
      "loss: 0.00042136115371249616\n",
      "loss: 0.0029170834459364414\n",
      "loss: 0.0003859865537378937\n",
      "loss: 0.003283559810370207\n",
      "loss: 0.0058214617893099785\n",
      "loss: 0.0023478574585169554\n",
      "loss: 0.0005806484841741621\n",
      "loss: 0.0003348570899106562\n",
      "loss: 1.3230155673227273e-06\n",
      "loss: 0.003758501261472702\n",
      "loss: 0.00023814458108972758\n",
      "loss: 0.003920846618711948\n",
      "loss: 1.597171149114729e-06\n",
      "loss: 0.0014997313264757395\n",
      "loss: 0.0022812075912952423\n",
      "loss: 0.004647218622267246\n",
      "loss: 0.0016541138757020235\n",
      "loss: 0.004778312519192696\n",
      "loss: 0.0009608071995899081\n",
      "loss: 5.337734546628781e-05\n",
      "loss: 0.004329511430114508\n",
      "loss: 0.0009037887793965638\n",
      "loss: 0.0024815259966999292\n",
      "loss: 0.003212674055248499\n",
      "loss: 0.0007854740251787007\n",
      "loss: 0.0001401230983901769\n",
      "loss: 0.004018579609692097\n",
      "loss: 0.0007691727369092405\n",
      "loss: 0.0041540260426700115\n",
      "loss: 0.00010720272257458419\n",
      "loss: 0.00021639661281369627\n",
      "loss: 0.0028800112195312977\n",
      "loss: 0.0006862329901196063\n",
      "loss: 0.002395531628280878\n",
      "loss: 0.00022186357819009572\n",
      "loss: 0.0038345414213836193\n",
      "loss: 9.04237967915833e-05\n",
      "loss: 0.002010013908147812\n",
      "loss: 0.0018649412086233497\n",
      "loss: 0.0026212872471660376\n",
      "loss: 0.006150409579277039\n",
      "loss: 0.0005098511464893818\n",
      "loss: 0.00401123845949769\n",
      "loss: 0.0014430044684559107\n",
      "loss: 0.0030064918100833893\n",
      "loss: 0.00010535995534155518\n",
      "loss: 0.0011130364146083593\n",
      "loss: 0.0012713309843093157\n",
      "loss: 0.005492899566888809\n",
      "loss: 8.094842632999644e-05\n",
      "loss: 0.004439357668161392\n",
      "loss: 0.0012567139929160476\n",
      "loss: 0.0032528445590287447\n",
      "loss: 0.0025747613981366158\n",
      "loss: 0.00020247542124707252\n",
      "loss: 0.0002589082869235426\n",
      "loss: 0.005165014415979385\n",
      "loss: 0.0019166882848367095\n",
      "loss: 0.019205531105399132\n",
      "loss: 0.008704525418579578\n",
      "loss: 0.005270625464618206\n",
      "loss: 0.003040818963199854\n",
      "loss: 0.0009387516183778644\n",
      "loss: 0.003350060898810625\n",
      "loss: 9.464516915613785e-05\n",
      "loss: 2.450858573865844e-06\n",
      "loss: 0.0007017407333478332\n",
      "loss: 0.0002318711776752025\n",
      "loss: 0.0008108025649562478\n",
      "loss: 0.0006700350786559284\n",
      "loss: 0.000279607338597998\n",
      "loss: 0.0011493152705952525\n",
      "loss: 0.005012026987969875\n",
      "loss: 0.002321647247299552\n",
      "loss: 0.0024146209470927715\n",
      "loss: 0.00031661527464166284\n",
      "loss: 0.00026833865558728576\n",
      "loss: 0.0012876562541350722\n",
      "loss: 0.0005972604267299175\n",
      "loss: 0.00019559430074878037\n",
      "loss: 0.0038545187562704086\n",
      "loss: 0.007728497497737408\n",
      "loss: 0.0005562530132010579\n",
      "loss: 0.000432405446190387\n",
      "loss: 2.6315206923754886e-05\n",
      "loss: 0.0008458697120659053\n",
      "loss: 0.0008568864432163537\n",
      "loss: 0.0006922576576471329\n",
      "loss: 0.00012426896137185395\n",
      "loss: 2.3911979951662943e-05\n",
      "loss: 0.0001469893759349361\n",
      "loss: 0.00023919779050629586\n",
      "loss: 0.0023344620130956173\n",
      "loss: 0.0005321624921634793\n",
      "loss: 0.0016369391232728958\n",
      "loss: 0.0016818114090710878\n",
      "loss: 0.004825008101761341\n",
      "loss: 0.0010456935269758105\n",
      "loss: 0.002206163015216589\n",
      "loss: 9.41450271056965e-05\n",
      "loss: 0.0010524217505007982\n",
      "loss: 0.0011528354370966554\n",
      "loss: 0.00021481746807694435\n",
      "loss: 0.0008601784938946366\n",
      "loss: 0.0019506908720359206\n",
      "loss: 0.010312923230230808\n",
      "loss: 0.0009426766773685813\n",
      "loss: 0.0023109856992959976\n",
      "loss: 4.184825229458511e-05\n",
      "loss: 0.002120640594512224\n",
      "loss: 0.0022872458212077618\n",
      "loss: 0.001171836513094604\n",
      "loss: 0.00017862854292616248\n",
      "loss: 0.0014837058261036873\n",
      "loss: 0.0014293328858911991\n",
      "Epoch [4/500], Training Loss: 0.0021\n",
      "loss: 0.0028195008635520935\n",
      "loss: 0.002663402585312724\n",
      "loss: 0.0014123354339972138\n",
      "loss: 0.001695719314739108\n",
      "loss: 0.004377461038529873\n",
      "loss: 0.002712815534323454\n",
      "loss: 0.0011722501367330551\n",
      "loss: 0.005399425979703665\n",
      "loss: 0.0049856603145599365\n",
      "loss: 0.0026028137654066086\n",
      "loss: 0.00017910261522047222\n",
      "loss: 0.0015186223899945617\n",
      "loss: 0.00014911565813235939\n",
      "loss: 0.00024066197511274368\n",
      "loss: 0.0020942871924489737\n",
      "loss: 0.0025004057679325342\n",
      "loss: 0.004049306269735098\n",
      "loss: 0.0006127360393293202\n",
      "loss: 0.0006033230456523597\n",
      "loss: 0.00012413840158842504\n",
      "loss: 0.00021355986245907843\n",
      "loss: 0.0001762154424795881\n",
      "loss: 0.00025683530839160085\n",
      "loss: 0.0005781626678071916\n",
      "loss: 0.001990205142647028\n",
      "loss: 0.0011970099294558167\n",
      "loss: 0.010299156419932842\n",
      "loss: 0.005209154449403286\n",
      "loss: 0.0037151165306568146\n",
      "loss: 0.005546395666897297\n",
      "loss: 0.00033812347101047635\n",
      "loss: 0.004318471997976303\n",
      "loss: 0.002276600105687976\n",
      "loss: 0.0024547665379941463\n",
      "loss: 0.00033908599289134145\n",
      "loss: 6.327331357169896e-05\n",
      "loss: 0.0011182858143001795\n",
      "loss: 6.392505019903183e-05\n",
      "loss: 0.00237708305940032\n",
      "loss: 0.002288358984515071\n",
      "loss: 0.0004165649297647178\n",
      "loss: 4.195540168439038e-05\n",
      "loss: 9.737064829096198e-05\n",
      "loss: 0.001429791678674519\n",
      "loss: 0.006546591874212027\n",
      "loss: 0.0010102323722094297\n",
      "loss: 0.0002085248997900635\n",
      "loss: 5.629464430967346e-05\n",
      "loss: 0.0007184048881754279\n",
      "loss: 0.001334317377768457\n",
      "loss: 0.00013370813394431025\n",
      "loss: 0.0036936993710696697\n",
      "loss: 0.004156007431447506\n",
      "loss: 0.0039590271189808846\n",
      "loss: 0.0007718816632404923\n",
      "loss: 0.0002266443334519863\n",
      "loss: 0.0008601633599027991\n",
      "loss: 0.00036112830275669694\n",
      "loss: 0.0018587388331070542\n",
      "loss: 0.0005614048568531871\n",
      "loss: 0.0006172609282657504\n",
      "loss: 0.0012342811096459627\n",
      "loss: 0.0010102398227900267\n",
      "loss: 0.00038071657763794065\n",
      "loss: 0.007563336286693811\n",
      "loss: 0.00034870914532803\n",
      "loss: 0.006991920527070761\n",
      "loss: 0.002923215739428997\n",
      "loss: 0.00043835508404299617\n",
      "loss: 0.000971104484051466\n",
      "loss: 0.0003467944625299424\n",
      "loss: 0.0005936776287853718\n",
      "loss: 0.0009607003885321319\n",
      "loss: 0.0017610788345336914\n",
      "loss: 0.0039051517378538847\n",
      "loss: 0.00013606611173599958\n",
      "loss: 0.0020676585845649242\n",
      "loss: 0.0033808164298534393\n",
      "loss: 0.00013185215357225388\n",
      "loss: 0.00016028061509132385\n",
      "loss: 0.0007582472753711045\n",
      "loss: 0.0015559261664748192\n",
      "loss: 0.0042825425043702126\n",
      "loss: 0.0020966981537640095\n",
      "loss: 0.009954603388905525\n",
      "loss: 9.118354500969872e-05\n",
      "loss: 0.0010256965178996325\n",
      "loss: 0.0019751351792365313\n",
      "loss: 3.908533835783601e-05\n",
      "loss: 0.0002638183650560677\n",
      "loss: 0.004112496040761471\n",
      "loss: 0.0017301688203588128\n",
      "loss: 0.0014160557184368372\n",
      "loss: 0.0020323866046965122\n",
      "loss: 0.0028220447711646557\n",
      "loss: 6.633873272221535e-05\n",
      "loss: 7.538363570347428e-05\n",
      "loss: 7.18479132046923e-05\n",
      "loss: 0.003477002028375864\n",
      "loss: 0.009318225085735321\n",
      "loss: 0.00438540056347847\n",
      "loss: 0.0018202621722593904\n",
      "loss: 0.00010538804053794593\n",
      "loss: 0.0013328721979632974\n",
      "loss: 0.00015922944294288754\n",
      "loss: 0.005276990123093128\n",
      "loss: 0.0013484228402376175\n",
      "loss: 0.0002971798530779779\n",
      "loss: 0.00022888365492690355\n",
      "loss: 0.00233044964261353\n",
      "loss: 0.0019810902886092663\n",
      "loss: 0.00022140602231957018\n",
      "loss: 0.0010122654493898153\n",
      "loss: 0.0007753930985927582\n",
      "loss: 8.128448098432273e-05\n",
      "loss: 0.0011559916893020272\n",
      "loss: 0.0037702538538724184\n",
      "loss: 0.0027348906733095646\n",
      "loss: 0.002150994725525379\n",
      "loss: 0.0010696107055991888\n",
      "loss: 0.0011696453439071774\n",
      "loss: 0.00026637769769877195\n",
      "loss: 0.001583069795742631\n",
      "loss: 0.0011029603192582726\n",
      "loss: 0.00013333292736206204\n",
      "loss: 0.0028748023323714733\n",
      "loss: 0.00322520243935287\n",
      "loss: 0.0017715729773044586\n",
      "loss: 2.6891479137702845e-05\n",
      "loss: 0.0025874481070786715\n",
      "loss: 0.0012886568438261747\n",
      "loss: 4.41366973973345e-05\n",
      "loss: 0.00021045799076091498\n",
      "loss: 0.0020765222143381834\n",
      "loss: 3.270279194111936e-06\n",
      "loss: 0.00030741249793209136\n",
      "loss: 0.0031980823259800673\n",
      "loss: 4.6436285629170015e-05\n",
      "loss: 0.004174917936325073\n",
      "loss: 0.0007502342923544347\n",
      "loss: 0.006545479409396648\n",
      "loss: 8.876623905962333e-05\n",
      "loss: 0.0024493413511663675\n",
      "loss: 0.0004084751126356423\n",
      "loss: 0.0008375003817491233\n",
      "loss: 0.003115198342129588\n",
      "loss: 0.0004101911617908627\n",
      "loss: 0.008263480849564075\n",
      "loss: 0.003179531078785658\n",
      "loss: 0.009220813401043415\n",
      "loss: 0.0005879517993889749\n",
      "loss: 0.00035985856084153056\n",
      "loss: 0.0035765673965215683\n",
      "loss: 0.0001701806322671473\n",
      "loss: 0.0015613905852660537\n",
      "loss: 0.0007228344911709428\n",
      "loss: 0.0012973274569958448\n",
      "loss: 0.0007664341828785837\n",
      "loss: 0.001496500219218433\n",
      "loss: 0.00032117284717969596\n",
      "loss: 0.005309362895786762\n",
      "loss: 0.0011636994313448668\n",
      "loss: 0.0006271314923651516\n",
      "loss: 0.0007730692741461098\n",
      "loss: 0.0011611200170591474\n",
      "loss: 0.01360403560101986\n",
      "loss: 0.0009546009823679924\n",
      "loss: 0.0023922037798911333\n",
      "loss: 0.003414312843233347\n",
      "loss: 0.0008625603513792157\n",
      "loss: 0.0018054438987746835\n",
      "loss: 0.0022486322559416294\n",
      "loss: 0.012990197166800499\n",
      "loss: 0.002707108622416854\n",
      "loss: 0.0014751164708286524\n",
      "loss: 0.0035253376699984074\n",
      "loss: 4.857966996496543e-05\n",
      "loss: 0.0001330910308752209\n",
      "loss: 0.00021187376114539802\n",
      "loss: 0.000887558504473418\n",
      "loss: 0.0017529778415337205\n",
      "loss: 0.00022745374008081853\n",
      "loss: 0.0009686289122328162\n",
      "loss: 0.0014527308521792293\n",
      "loss: 0.00019719931879080832\n",
      "loss: 0.00025480781914666295\n",
      "loss: 0.007706864736974239\n",
      "loss: 0.00017871253658086061\n",
      "loss: 6.450637738453224e-05\n",
      "loss: 0.0002453435445204377\n",
      "loss: 0.00042052441858686507\n",
      "loss: 0.007792389951646328\n",
      "loss: 0.00034979754127562046\n",
      "loss: 0.0002673208655323833\n",
      "loss: 0.006844315677881241\n",
      "loss: 7.63192838348914e-06\n",
      "loss: 0.006618353072553873\n",
      "loss: 5.795077231596224e-05\n",
      "loss: 0.00590917095541954\n",
      "loss: 0.001119497581385076\n",
      "loss: 0.0028128386475145817\n",
      "loss: 0.0003251238085795194\n",
      "loss: 0.0019144571851938963\n",
      "loss: 0.00020646661869250238\n",
      "loss: 0.003925357945263386\n",
      "loss: 8.574353705625981e-05\n",
      "loss: 0.008643276989459991\n",
      "loss: 0.0007326591876335442\n",
      "loss: 0.00613063108175993\n",
      "loss: 0.0085997823625803\n",
      "Epoch [5/500], Training Loss: 0.0021\n",
      "loss: 0.0028136763721704483\n",
      "loss: 0.000342634943081066\n",
      "loss: 0.003027521539479494\n",
      "loss: 0.0008567790500819683\n",
      "loss: 0.004922997672110796\n",
      "loss: 0.005183405242860317\n",
      "loss: 0.004961705766618252\n",
      "loss: 5.542438157135621e-05\n",
      "loss: 0.00011136491229990497\n",
      "loss: 0.00029773791902698576\n",
      "loss: 0.0003664923715405166\n",
      "loss: 0.0010561470407992601\n",
      "loss: 0.001323868753388524\n",
      "loss: 0.003247962100431323\n",
      "loss: 0.00035970433964394033\n",
      "loss: 0.0005777613841928542\n",
      "loss: 0.005645803641527891\n",
      "loss: 0.0019165059784427285\n",
      "loss: 0.0067870416678488255\n",
      "loss: 0.007209651172161102\n",
      "loss: 9.402514842804521e-05\n",
      "loss: 0.0011719729518517852\n",
      "loss: 0.0004581069224514067\n",
      "loss: 0.007619839161634445\n",
      "loss: 0.0003094772982876748\n",
      "loss: 0.0023626177571713924\n",
      "loss: 0.001045191427692771\n",
      "loss: 0.015276739373803139\n",
      "loss: 0.004065686371177435\n",
      "loss: 0.00016265844169538468\n",
      "loss: 0.005846343003213406\n",
      "loss: 0.0007077931659296155\n",
      "loss: 0.002044465858489275\n",
      "loss: 0.0007994025945663452\n",
      "loss: 0.0006059993174858391\n",
      "loss: 1.3701469470106531e-05\n",
      "loss: 0.00406118668615818\n",
      "loss: 0.00046811631182208657\n",
      "loss: 0.0016853127162903547\n",
      "loss: 0.0035821981728076935\n",
      "loss: 0.00044403597712516785\n",
      "loss: 0.000723853416275233\n",
      "loss: 0.0008853558683767915\n",
      "loss: 0.0043614888563752174\n",
      "loss: 0.008536718785762787\n",
      "loss: 0.004602337256073952\n",
      "loss: 0.0008396154153160751\n",
      "loss: 0.0005069031612947583\n",
      "loss: 0.00010491973080206662\n",
      "loss: 0.0001280061696888879\n",
      "loss: 0.005454992409795523\n",
      "loss: 0.002802221802994609\n",
      "loss: 0.004435284994542599\n",
      "loss: 0.004040216561406851\n",
      "loss: 0.0002392595197306946\n",
      "loss: 0.00046314235078170896\n",
      "loss: 0.0002583637251518667\n",
      "loss: 0.0038409780245274305\n",
      "loss: 0.00021215106244198978\n",
      "loss: 0.003909280989319086\n",
      "loss: 0.0002810667792800814\n",
      "loss: 0.0005768745904788375\n",
      "loss: 0.0012428032932803035\n",
      "loss: 0.004980917554348707\n",
      "loss: 0.002081636805087328\n",
      "loss: 0.0010790558299049735\n",
      "loss: 7.886093226261437e-05\n",
      "loss: 0.0001765040506143123\n",
      "loss: 0.0010074656456708908\n",
      "loss: 0.01036415621638298\n",
      "loss: 0.0024122032336890697\n",
      "loss: 0.0004109441360924393\n",
      "loss: 0.00015779004024807364\n",
      "loss: 0.00012176598102087155\n",
      "loss: 7.98342443886213e-05\n",
      "loss: 0.004809802398085594\n",
      "loss: 0.0008496457594446838\n",
      "loss: 0.0007696236716583371\n",
      "loss: 0.0005914306384511292\n",
      "loss: 0.002872366923838854\n",
      "loss: 0.0004967927234247327\n",
      "loss: 0.00030926187173463404\n",
      "loss: 0.006210047286003828\n",
      "loss: 5.8660116337705404e-05\n",
      "loss: 0.007248672656714916\n",
      "loss: 0.0029171656351536512\n",
      "loss: 0.00313555751927197\n",
      "loss: 0.011811292730271816\n",
      "loss: 0.000444056058768183\n",
      "loss: 0.0005438976222649217\n",
      "loss: 0.006506301462650299\n",
      "loss: 0.007923954166471958\n",
      "loss: 0.0003759617102332413\n",
      "loss: 0.00017901379032991827\n",
      "loss: 8.607840572949499e-05\n",
      "loss: 0.000337214267347008\n",
      "loss: 0.0010357904247939587\n",
      "loss: 0.0043267617002129555\n",
      "loss: 0.005905113182961941\n",
      "loss: 0.0011190851218998432\n",
      "loss: 3.790878054132918e-06\n",
      "loss: 0.0006472402601502836\n",
      "loss: 0.0032772920094430447\n",
      "loss: 0.00035758365993387997\n",
      "loss: 0.00027815322391688824\n",
      "loss: 0.0012299668742343783\n",
      "loss: 0.0004366464854683727\n",
      "loss: 0.0007727793999947608\n",
      "loss: 0.0009174367296509445\n",
      "loss: 0.0031728532630950212\n",
      "loss: 0.0012003392912447453\n",
      "loss: 0.0002570629003457725\n",
      "loss: 0.0001838330936152488\n",
      "loss: 0.0025921163614839315\n",
      "loss: 0.0002081280545098707\n",
      "loss: 0.00014008316793479025\n",
      "loss: 5.581274308497086e-05\n",
      "loss: 0.00012902729213237762\n",
      "loss: 0.00015659673954360187\n",
      "loss: 0.0005192197277210653\n",
      "loss: 0.0023447913117706776\n",
      "loss: 0.0001807458174880594\n",
      "loss: 0.00522899953648448\n",
      "loss: 0.0009946932550519705\n",
      "loss: 0.0013692311476916075\n",
      "loss: 0.007312767207622528\n",
      "loss: 0.003911983221769333\n",
      "loss: 0.00385289010591805\n",
      "loss: 0.005247634369879961\n",
      "loss: 0.0063413819298148155\n",
      "loss: 0.001277943723835051\n",
      "loss: 0.0008047324372455478\n",
      "loss: 0.003349360078573227\n",
      "loss: 0.0015436038374900818\n",
      "loss: 0.0008994013769552112\n",
      "loss: 0.00014588140766136348\n",
      "loss: 0.00025042990455403924\n",
      "loss: 2.698222488106694e-05\n",
      "loss: 0.0002789848658721894\n",
      "loss: 0.004433170426636934\n",
      "loss: 0.001919361180625856\n",
      "loss: 0.0007530624279752374\n",
      "loss: 0.0032281826715916395\n",
      "loss: 0.00037757103564217687\n",
      "loss: 0.0002832726459018886\n",
      "loss: 0.014774682000279427\n",
      "loss: 0.00015921573503874242\n",
      "loss: 0.00031747520552016795\n",
      "loss: 0.0035882852971553802\n",
      "loss: 0.0054077645763754845\n",
      "loss: 0.0038069519214332104\n",
      "loss: 0.002324350643903017\n",
      "loss: 0.00020615544053725898\n",
      "loss: 0.005341788753867149\n",
      "loss: 0.0045825885608792305\n",
      "loss: 0.0001009181869449094\n",
      "loss: 0.0026363797951489687\n",
      "loss: 0.0002508407342247665\n",
      "loss: 0.0002252745471196249\n",
      "loss: 0.0005139197455719113\n",
      "loss: 3.277089126640931e-05\n",
      "loss: 0.00026917114155367017\n",
      "loss: 0.00040042150067165494\n",
      "loss: 0.00017041712999343872\n",
      "loss: 7.488716801162809e-05\n",
      "loss: 2.9271259336383082e-05\n",
      "loss: 0.0016044506337493658\n",
      "loss: 0.002802359638735652\n",
      "loss: 0.0008408124558627605\n",
      "loss: 0.005657707341015339\n",
      "loss: 0.0035338536836206913\n",
      "loss: 0.00037944718496873975\n",
      "loss: 0.00022784143220633268\n",
      "loss: 0.001774005824699998\n",
      "loss: 0.00024217445752583444\n",
      "loss: 2.5038380044861697e-05\n",
      "loss: 0.0012298047076910734\n",
      "loss: 6.267066055443138e-05\n",
      "loss: 0.003902292577549815\n",
      "loss: 0.0013320643920451403\n",
      "loss: 0.0014534492511302233\n",
      "loss: 0.0002494728541933\n",
      "loss: 0.0012353456113487482\n",
      "loss: 0.000387880252674222\n",
      "loss: 0.005395636893808842\n",
      "loss: 0.000683712656609714\n",
      "loss: 0.00044580185203813016\n",
      "loss: 0.0017365257954224944\n",
      "loss: 0.004410776775330305\n",
      "loss: 1.2737976931020967e-06\n",
      "loss: 0.00023426240659318864\n",
      "loss: 0.000667168409563601\n",
      "loss: 0.0028988118283450603\n",
      "loss: 0.00010977381316479295\n",
      "loss: 0.00013979467621538788\n",
      "loss: 0.0009566317312419415\n",
      "loss: 0.0009574275463819504\n",
      "loss: 0.0003710219170898199\n",
      "loss: 0.008606841787695885\n",
      "loss: 7.840614125598222e-05\n",
      "loss: 0.005685956683009863\n",
      "loss: 0.002686034422367811\n",
      "loss: 9.048291758517735e-06\n",
      "loss: 0.001539589837193489\n",
      "loss: 0.003409912809729576\n",
      "loss: 9.102726471610367e-05\n",
      "loss: 0.0005081391427665949\n",
      "loss: 0.0005450336029753089\n",
      "loss: 0.0009452454978600144\n",
      "loss: 0.0031425426714122295\n",
      "Epoch [6/500], Training Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/internal_util.py\", line 48, in run\n",
      "    self._run()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/internal_util.py\", line 99, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/internal.py\", line 327, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 398, in send\n",
      "    send_handler(record)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 420, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 1224, in send_request_summary_record\n",
      "    self._update_summary_record(record.request.summary_record.summary)\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 1218, in _update_summary_record\n",
      "    self._update_summary()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/internal/sender.py\", line 1238, in _update_summary\n",
      "    with open(summary_path, \"w\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/Users/ssongjinseobgmail.com/coding/Re-Beethoven/src/wandb/run-20241016_054054-5m5c0kyy/files/wandb-summary.json'\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m         epoch_test_loss \u001b[38;5;241m=\u001b[39m running_loss_test \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dLoader_test)\n\u001b[1;32m     72\u001b[0m         test_losses\u001b[38;5;241m.\u001b[39mappend(epoch_test_loss)\n\u001b[0;32m---> 73\u001b[0m         \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mepoch_test_loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_test_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# At the end, you can plot the losses if needed\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:452\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:403\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:1930\u001b[0m, in \u001b[0;36mRun.log\u001b[0;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1924\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[1;32m   1925\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1926\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_define_metric\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1927\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon how to customize your x-axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1928\u001b[0m         repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1929\u001b[0m     )\n\u001b[0;32m-> 1930\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:1651\u001b[0m, in \u001b[0;36mRun._log\u001b[0;34m(self, data, step, commit)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1651\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:1523\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[0;34m(self, row, step, commit)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[1;32m   1521\u001b[0m     not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1523\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/interface/interface.py:644\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[0;34m(self, data, user_step, step, flush, publish_step, run)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flush \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m     partial_history\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mflush \u001b[38;5;241m=\u001b[39m flush\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_partial_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/interface/interface_shared.py:89\u001b[0m, in \u001b[0;36mInterfaceShared._publish_partial_history\u001b[0;34m(self, partial_history)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_partial_history\u001b[39m(\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m, partial_history: pb\u001b[38;5;241m.\u001b[39mPartialHistoryRequest\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(partial_history\u001b[38;5;241m=\u001b[39mpartial_history)\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/interface/interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    219\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[1;32m    220\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    150\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/wandb/sdk/lib/sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "wandb.init(project=\"audio-gen\", config={\n",
    "    \"epochs\": num_epochs,\n",
    "    \"batch_size\": len(dLoader),  # assuming dLoader gives one batch per step\n",
    "    \"learning_rate\": LR,\n",
    "    \"device\": \"cuda RTX 3080 Ti\",\n",
    "    \"ck_len\": ck_len,\n",
    "    \"num_blocks\": 6,\n",
    "    },\n",
    "    name=f\"run_{current_time}\"\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for i,(inputs, labels) in enumerate(dLoader):\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        if torch.any(torch.isnan(inputs)) or torch.any(torch.isnan(labels)):\n",
    "            print(\"Input or labels contain NaN values.\")\n",
    "            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "        optimizer.step()\n",
    "        wandb.log({'single_loss': loss.item()})\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dLoader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    wandb.log({\"epoch_train_loss\": epoch_loss})\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'models/modelDict_epoch_{epoch+1}.pth')\n",
    "        torch.save(model, f'models/model_epoch_{epoch+1}.pth')\n",
    "        model.eval() \n",
    "        running_loss_test = 0.0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in dLoader_test:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                for name, param in model.named_parameters():\n",
    "                    if param.grad is not None and torch.any(torch.isnan(param.grad)):\n",
    "                        print(f\"Gradient for {name} contains NaN values.\")\n",
    "                        \n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss_test += loss.item()\n",
    "         \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Average validation loss for the epoch\n",
    "        epoch_test_loss = running_loss_test / len(dLoader_test)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        wandb.log({\"eval_loss\":epoch_test_loss})    \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_test_loss:.4f}')\n",
    "    \n",
    "# At the end, you can plot the losses if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "from layers.main_model import net\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = torch.load(\"model_epoch_45.pth\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Assuming dLoader_test is your test dataloader and each input/label has shape (1, L)\n",
    "output_folder = \"eval_audio\"  # Folder to store audio files\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch_idx, (inputs, labels) in enumerate(dLoader_test):\n",
    "        inputs = inputs.to(device)  # Shape (1, L)\n",
    "        labels = labels.to(device)  # Shape (1, L)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Shape (1, L)\n",
    "\n",
    "        # Concatenate inputs, labels, and outputs\n",
    "        concatenated_audio = torch.cat((inputs, outputs,labels), dim=1)  # Shape (1, 3*L)\n",
    "\n",
    "        # Convert to CPU and detach (if necessary)\n",
    "        concatenated_audio = concatenated_audio.cpu().detach()\n",
    "\n",
    "        # Save as audio (normalize to [-1, 1] if needed)\n",
    "        file_name = f\"{output_folder}/audio_batch_{batch_idx}.wav\"\n",
    "        torchaudio.save(file_name, concatenated_audio, sample_rate=SR)\n",
    "\n",
    "        print(f\"Saved concatenated audio for batch {batch_idx} to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from layers.main_model import net\n",
    "def print_model_info(model):\n",
    "    # Calculate total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Assuming parameters are stored as float32 (4 bytes per parameter)\n",
    "    param_size_bytes = total_params * 4  # 4 bytes for float32\n",
    "    param_size_gb = param_size_bytes / (1024 ** 3)  # Convert to GB\n",
    "    \n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "    print(f\"Model size: {param_size_gb:.2f} GB\")\n",
    "\n",
    "# Example usage\n",
    "model = net(sequence_length=8000*30, num_blocks=12, activation='silu',lstm_option=True)\n",
    "print_model_info(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
